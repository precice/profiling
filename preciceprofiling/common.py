#! /usr/bin/env python3

# Import the currently fastest json library
import ujson as json

import csv
import datetime
import functools
import sys
from collections import namedtuple
import matplotlib.pyplot as plt
import polars as pl


# Version of the profiling files generated by preCICE
RUN_FILE_VERSION: int = 1
# Version of the merged profiling data generated by merge
MERGED_FILE_VERSION: int = 1


def mergedDict(dict1, dict2):
    merged = dict1.copy()
    merged.update(dict2)
    return merged


def warning(message, filename=None):
    prefix = f"{filename}: " if filename else ""
    print(f"{prefix}\033[33mwarning:\033[0m {message}")


def printWide(df):
    from itertools import repeat

    def makeLabel(eventName):
        if "_GLOBAL" in eventName:
            return "total"

        indentation = "  "

        if "/" not in eventName:
            return indentation + eventName
        parts = eventName.split("/")
        return indentation * len(parts) + parts[-1]

    df: pl.DataFrame = (
        df.sort("eid")
        .with_columns(pl.col("eid").map_elements(makeLabel, return_dtype=pl.String))
        .rename({"eid": "name"})
    )

    blocksize = 6
    assert len(df.columns) % blocksize == 1

    headerFmts = []
    bodyFmts = []
    colWidths = []
    for col in df.iter_columns():
        headerWidth = len(col.name.encode())

        def fmt(d):
            if col.dtype == pl.Float32 or col.dtype == pl.Float64:
                return len(f"{d:.2f}")
            else:
                return len(f"{d}")

        width = col.map_elements(fmt, return_dtype=pl.Int64).max()
        width = max(width, headerWidth)
        colWidths.append(width)
        sw = str(width)

        headerFmts.append("{:<" + sw + "}")

        if col.dtype == pl.Float32 or col.dtype == pl.Float64:
            bodyFmts.append("{:>" + sw + ".2f}")
        elif col.dtype == pl.String:
            bodyFmts.append("{:<" + sw + "}")
        else:
            bodyFmts.append("{:>" + sw + "}")

    headerFmt = ""
    hline = ""
    bodyFmt = ""

    for col, (h, b, w) in enumerate(zip(headerFmts, bodyFmts, colWidths)):
        if col % blocksize == 1:
            headerFmt += " │ "
        else:
            headerFmt += " "
        headerFmt += h

        if col % blocksize == 1:
            hline += "─┼─"
        else:
            hline += " "

        hline += "─" * w

        if col % blocksize == 1:
            bodyFmt += " │ "
        else:
            bodyFmt += " "
        bodyFmt += b

    print(headerFmt.format(*(df.columns)))
    print(hline)
    for row in df.iter_rows():
        print(
            bodyFmt.format(
                *map(lambda e: float("nan") if e is None else e, row)
            ).replace("nan", "   ")
        )


@functools.lru_cache
def ns_to_unit_factor(unit):
    return {
        "ns": 1,
        "us": 1e-3,
        "ms": 1e-6,
        "s": 1e-9,
        "m": 1e-9 / 60,
        "h": 1e-9 / 3600,
    }[unit]


class RankData:
    def __init__(self, data):
        meta = data["meta"]
        self.name = meta["name"]
        self.rank = meta["rank"]
        self.size = meta["size"]
        self.unix_us = meta["unix_us"]
        self.tinit = meta["tinit"]

        self.events = data["events"]

    @property
    def type(self):
        return "Primary (0)" if self.rank == 0 else f"Secondary ({self.rank})"

    def toListOfTuples(self, eventLookup):
        for e in self.events:
            yield (
                self.name,
                self.rank,
                eventLookup[e["eid"]],
                int(e["ts"]),
                int(e["dur"]),
            )


class Run:
    def __init__(self, filename):
        print(f"Reading events file {filename}")
        import json

        with open(filename, "r") as f:
            content = json.load(f)

        if "file_version" not in content:
            warning(
                "The file doesn't contain a version (preCICE version v3.2 or earlier) and may be incompatible.",
                filename,
            )
        elif (version := content["file_version"]) != MERGED_FILE_VERSION:
            warning(
                f"The file uses version {version}, which doesn't match the expected version {MERGED_FILE_VERSION} and may be incompatible.",
                filename,
            )

        self.eventLookup = {int(id): name for id, name in content["eventDict"].items()}
        self.data = content["events"]

    def iterRanks(self):
        for pranks in self.data.values():
            for d in sorted(
                pranks.values(), key=lambda data: int(data["meta"]["rank"])
            ):
                yield RankData(d)

    def iterParticipant(self, name):
        for d in self.data[name].values():
            yield RankData(d)

    def participants(self):
        return self.data.keys()

    def lookupEvent(self, id):
        return self.eventLookup[int(id)]

    def toTrace(self, selectRanks):
        if selectRanks:
            print(f'Selected ranks: {",".join(map(str,sorted(selectRanks)))}')

        def filterop(rank):
            return True if not selectRanks else rank.rank in selectRanks

        pids = {name: id for id, name in enumerate(self.participants())}
        tids = {
            (rank.name, rank.rank): id
            for id, rank in enumerate(filter(filterop, self.iterRanks()))
        }
        metaEvents = [
            {"name": "process_name", "ph": "M", "pid": pid, "args": {"name": name}}
            for name, pid in pids.items()
        ] + [
            {
                "name": "thread_name",
                "ph": "M",
                "pid": pid,
                "tid": tids[(rank.name, rank.rank)],
                "args": {"name": rank.type},
            }
            for name, pid in pids.items()
            for rank in filter(filterop, self.iterParticipant(name))
        ]

        mainEvents = []
        for rank in filter(filterop, self.iterRanks()):
            pid, tid = pids[rank.name], tids[(rank.name, rank.rank)]
            for e in rank.events:
                en = self.lookupEvent(e["eid"])
                mainEvents.append(
                    mergedDict(
                        {
                            "name": en,
                            "cat": "Solver" if en.startswith("solver") else "preCICE",
                            "ph": "X",  # complete event
                            "pid": pid,
                            "tid": tid,
                            "ts": e["ts"],
                            "dur": e["dur"],
                        },
                        {} if "data" not in e else {"args": e["data"]},
                    )
                )

        return {"traceEvents": metaEvents + mainEvents}

    def allDataFields(self):
        return list(
            {
                dname
                for rank in self.iterRanks()
                for e in rank.events
                if "data" in e
                for dname in e["data"].keys()
            }
        )

    def toExportList(self, unit, dataNames):
        factor = ns_to_unit_factor(unit) * 1e3 if unit else 1

        def makeData(e):
            if "data" not in e:
                return tuple("" for dname in dataNames)

            return tuple(e["data"].get(dname, "") for dname in dataNames)

        for rank in self.iterRanks():
            for e in rank.events:
                yield (
                    rank.name,
                    rank.rank,
                    rank.size,
                    self.lookupEvent(e["eid"]),
                    e["ts"],
                    e["dur"] * factor,
                ) + makeData(e)

    def toDataFrame(self):
        import itertools

        columns = ["participant", "rank", "eid", "ts", "dur"]
        df = pl.DataFrame(
            data=itertools.chain.from_iterable(
                map(lambda r: r.toListOfTuples(self.eventLookup), self.iterRanks())
            ),
            schema=[
                ("participant", pl.Utf8),
                ("rank", pl.Int32),
                ("eid", pl.Utf8),
                ("ts", pl.Int64),
                ("dur", pl.Int64),
            ],
        ).with_columns([pl.col("ts").cast(pl.Datetime("us"))])
        return df


def traceCommand(profilingfile, outfile, rankfilter, limit):
    run = Run(profilingfile)
    selection = (
        set()
        .union(rankfilter if rankfilter else [])
        .union(range(limit) if limit else [])
    )
    traces = run.toTrace(selection)
    print(f"Writing to {outfile}")
    with open(outfile, "w") as outfile:
        json.dump(traces, outfile)
    return 0


def exportCommand(profilingfile, outfile, unit):
    run = Run(profilingfile)
    dataFields = run.allDataFields()
    fieldnames = [
        "participant",
        "rank",
        "size",
        "event",
        "timestamp",
        "duration",
    ] + dataFields
    print(f"Writing to {outfile}")
    with open(outfile, "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(fieldnames)
        writer.writerows(run.toExportList(unit, dataFields))
    return 0


def analyzeCommand(profilingfile, participant, event, outfile=None, unit="us"):
    run = Run(profilingfile)
    df = run.toDataFrame()

    participants = set(df.get_column("participant").unique())
    assert (
        participant in participants
    ), f"Given participant {participant} doesn't exist. Known: " + ", ".join(
        participants
    )

    print(f"Output timing are in {unit}.")

    # Filter by participant
    # Convert duration to requested unit
    dur_factor = 1000 * ns_to_unit_factor(unit)
    df = (
        df.filter(pl.col("participant") == participant)
        .drop("participant")
        .with_columns(
            (pl.col("dur") * dur_factor),
            (pl.col("dur") / pl.col("dur").max().over(["rank"]) * 100).alias("rel"),
        )
    )

    ranks = df.select("rank").unique()

    if len(ranks) == 1:
        joined = (
            df.group_by("eid")
            .agg(
                pl.sum("dur").alias("sum"),
                pl.sum("rel").alias("%"),
                pl.count("dur").alias("count"),
                pl.mean("dur").alias("mean"),
                pl.min("dur").alias("min"),
                pl.max("dur").alias("max"),
            )
            .sort("eid")
        )
    else:
        ldf = df.lazy()
        rankAdvance = (
            ldf.filter((pl.col("eid") == event) & (pl.col("rank") > 0))
            .group_by("rank")
            .agg(pl.col("dur").sum())
            .sort("dur")
            .collect()
        )
        minSecRank = rankAdvance.select(pl.first("rank")).item()
        maxSecRank = rankAdvance.select(pl.last("rank")).item()

        if minSecRank is None or maxSecRank is None:
            ranksToPrint = (0,)
            print(
                "Selection only contains the primary rank 0 as event isn't available on secondary ranks."
            )
        elif minSecRank == maxSecRank:
            ranksToPrint = (0, minSecRank)
            print(
                f"Selection contains the primary rank 0 and secondary rank {minSecRank}."
            )
        else:
            ranksToPrint = (0, minSecRank, maxSecRank)
            print(
                f"Selection contains the primary rank 0, the cheapest secondary rank {minSecRank}, and the most expensive secondary rank {maxSecRank}."
            )

        ldf = df.lazy()
        joined = (
            pl.concat(
                [
                    (
                        ldf.filter(pl.col("rank") == rank)
                        .group_by("eid")
                        .agg(
                            pl.sum("dur").alias(f"R{rank}:sum"),
                            pl.sum("rel").alias(f"R{rank}:%"),
                            pl.count("dur").alias(f"R{rank}:count"),
                            pl.mean("dur").alias(f"R{rank}:mean"),
                            pl.min("dur").alias(f"R{rank}:min"),
                            pl.max("dur").alias(f"R{rank}:max"),
                        )
                    )
                    for rank in ranksToPrint
                ],
                how="align",
            )
            .sort("eid")
            .collect()
        )

    printWide(joined)

    if outfile:
        print(f"Writing to {outfile}")
        joined.write_csv(outfile)

    return 0


def histogramCommand(profilingfile, outfile, participant, event, rank, bins, unit="us"):
    run = Run(profilingfile)
    df = run.toDataFrame()

    # Check user input
    assert df.select(
        pl.col("participant").is_in([participant]).any()
    ).item(), f"Given participant {participant} doesn't exist."
    assert df.select(
        pl.col("eid").is_in([event]).any()
    ).item(), f"Given event {event} doesn't exist."
    if not rank is None:
        assert df.select(
            pl.col("rank").is_in([rank]).any()
        ).item(), f"Given rank {rank} doesn't exist."

    # Filter by participant and event
    filter = (pl.col("participant") == participant) & (pl.col("eid") == event)
    # Optionally filter by rank
    if not rank is None:
        filter = filter & (pl.col("rank") == rank)

    # duration scaling factor
    dur_factor = 1000 * ns_to_unit_factor(unit)

    # Query durations
    durations = df.filter(filter).select(pl.col("dur") * dur_factor)

    ranks = df["rank"].unique()
    rankDesc = (
        "ranks: " + ",".join(map(str, ranks))
        if len(ranks) < 5
        else f"{len(ranks)} ranks"
    )

    fig, ax = plt.subplots(figsize=(14, 7), tight_layout=True)
    ax.set_title(f'Histogram of event "{event}" on {participant} ({rankDesc})')
    ax.set_xlabel(f"Duration [{unit}]")
    ax.set_ylabel("Occurrence")
    hist_data = ax.hist(durations, bins=bins, histtype="barstacked", align="mid")
    ax.bar_label(hist_data[2])
    binborders = hist_data[1]
    ax.set_xticks(binborders, labels=[f"{d:,.2f}" for d in binborders], rotation=90)
    ax.set_xlim(left=min(binborders), right=max(binborders))
    ax.grid(axis="x")

    if outfile:
        print(f"Writing to {outfile}")
        plt.savefig(outfile)
    else:
        plt.show()

    return 0
