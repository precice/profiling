#! /usr/bin/env python3

# Import the fastest jsons library available
try:
    import orjson as json
except ImportError:
    try:
        import ujson as json
    except ImportError:
        try:
            import simplejson as json
        except ImportError:
            import json

import argparse
import sys
from collections import namedtuple
import pathlib


# Version of the profiling files generated by preCICE
RUN_FILE_VERSION: int = 1
# Version of the merged profiling data generated by merge
MERGED_FILE_VERSION: int = 1


def mergedDict(dict1, dict2):
    merged = dict1.copy()
    merged.update(dict2)
    return merged


def warning(message, filename=None):
    prefix = f"{str(filename)}: " if filename else ""
    print(f"{prefix}\033[33mwarning:\033[0m {message}")


def readRobust(filename: pathlib.Path):
    content = filename.read_bytes()
    try:
        return json.loads(content)  # try direct
    except:
        warning("File damaged. Attempting to terminate truncated event file.", filename)
        content += b"]}"
        try:
            return json.loads(content)  # try terminated
        except:
            warning("Unable to load critically damaged file.", filename)
            return {}  # give up


def alignEvents(events):
    """Aligns passed events of multiple ranks and or participants.
    All ranks of a participant align at initialization, ensured by a barrier in preCICE.
    Primary ranks of all participants align after successfully establishing primary connections.
    """
    assert "events" in events and "eventDict" in events, "Not a loaded event file"
    assert len(events.get("events")) > 0, "No participants in the file"
    participants = events["events"].keys()
    grouped = events["events"]

    # Align ranks of each participant
    for participant, ranks in grouped.items():
        if len(ranks) == 1:
            continue
        print(f"Aligning {len(ranks)} ranks of {participant}")
        intraSyncID = [
            id
            for id, name in events["eventDict"].items()
            if "com.initializeIntraCom" in name
        ][0]
        syncs = {
            rank: e["ts"] + e["dur"]
            for rank, data in ranks.items()
            for e in data["events"]
            if e["eid"] == intraSyncID
        }

        firstSync = min(syncs.values())
        shifts = {rank: firstSync - tp for rank, tp in syncs.items()}

        for rank, data in ranks.items():
            for e in data["events"]:
                e["ts"] += shifts[rank]

    if len(grouped) == 1:
        return events

    # Align participants
    primaries = [name for name, ranks in events["events"].items() if 0 in ranks]

    for lonely in set(events["events"].keys()).difference(primaries):
        print(f"Cannot align {lonely} as event file of rank 0 is missing.")

    # Cannot align anything
    if len(primaries) == 1:
        return events

    # Find synchronization points
    syncEvents = [
        "m2n.acceptPrimaryRankConnection.",
        "m2n.requestPrimaryRankConnection.",
    ]
    # Event ID -> Remote name
    syncIDs = {
        id: name.rsplit(".", 1)[1]
        for id, name in events["eventDict"].items()
        if any([check in name for check in syncEvents])
    }
    syncs = {
        local: {
            remote: e["ts"] + e["dur"]
            for e in events["events"][local][0]["events"]
            for eid, remote in syncIDs.items()
            if eid == e["eid"]
        }
        for local in primaries
    }

    def hasSync(l, r):
        return (
            syncs.get(l)
            and syncs.get(l).get(r)
            and syncs.get(r)
            and syncs.get(r).get(l)
        )

    shifts = {
        (local, remote): syncs[local][remote] - syncs[remote][local]
        # all unique participant combinations
        for local in primaries
        for remote in primaries
        if local < remote
        if hasSync(local, remote)
    }
    for (local, remote), shift in shifts.items():
        print(f"Aligning {remote} ({shift}us) with {local}")
        for rank, data in events["events"][remote].items():
            data["meta"]["unix_us"] += shift
            for e in data["events"]:
                e["ts"] += shift

    return events


def groupEvents(events: [dict], initTime: int):

    nameMap = {int(e["eid"]): e["en"] for e in events if e["et"] == "n"}

    completed = []  # completed events
    active = []  # stack of active events
    stack = []  # stack of eids

    for event in events:
        type = event["et"]

        eid: int = event["eid"]
        assert isinstance(eid, int)

        # Handle event starts
        if type == "b":
            stack.append(eid)
            event["ts"] = int(event["ts"])
            event["eid"] = tuple(stack)  # all eids that make up the name
            active.append(event)
        # Handle event stops
        elif type == "e":
            assert (
                eid == active[-1]["eid"][-1]
            ), f"end of inactive event {nameMap[eid]}, found {nameMap[active[-1]['eid'][-1]]}"
            begin = active.pop()
            begin["dur"] = int(event["ts"]) - begin["ts"]
            begin["ts"] = int(begin["ts"]) + initTime
            begin.pop("et")
            completed.append(begin)
            assert (
                stack[-1] == eid
            ), f"Expected to end event {eid} but the currently active event is {nameMap[stack[-1]]}. Note that events need to follow a strict nesting and overlapping starts/stops are not permitted."
            stack.pop()
        # Handle event data
        elif type == "d":
            for e in reversed(active):
                if e["eid"] == eid:
                    d = active[eid].get("data", {})
                    dname = nameMap[event["dn"]]
                    assert isinstance(dname, str)
                    d[dname] = int(event["dv"])
                    active[eid]["data"] = d
                    break

    # Handle leftover events in case of a truncated input file
    if active:
        lastTS = min(map(lambda e: e["ts"] + e["dur"], completed))
        for event in active.values():
            eid = event["eid"]  # This is a global id
            print(f"Truncating event without end {eid}")
            begin = active[eid]
            begin["ts"] = int(begin["ts"]) + initTime
            begin["dur"] = lastTS - begin["ts"]
            begin.pop("et")
            completed.append(begin)

    assert not active

    assert all((isinstance(e["eid"], tuple) for e in completed))

    return nameMap, sorted(completed, key=lambda e: e["ts"])


def compressNames(events):
    globalNames = {}  # name to geid
    for pname, ranks in events.items():
        for ranks, rankdata in ranks.items():
            assert "events" in rankdata
            assert "meta" in rankdata
            rankNames = rankdata.pop("nameMap")
            rankToGlobal = {}

            for e in rankdata["events"]:
                eids = e["eid"]
                geid = None
                if ret := rankToGlobal.get(eids):
                    # already seen
                    geid = ret
                else:
                    name = "/".join(map(rankNames.get, eids))
                    if ret := globalNames.get(name):
                        # name exists in global DB
                        geid = ret
                    else:
                        geid = len(globalNames)
                        globalNames[name] = geid
                    # update local cache
                    rankToGlobal[eids] = geid
                e["eid"] = geid

    idToName = dict(zip(globalNames.values(), globalNames.keys()))
    return idToName


def loadProfilingOutputs(filenames: list[pathlib.Path]):
    # Load all jsons
    print("Loading event files")
    events = {}
    for fn in filenames:
        json = readRobust(fn)

        # General checks
        if not json:
            warning(
                "The file is empty or was unable to be load and will be ignored.", fn
            )
            continue
        if "meta" not in json:
            warning("The file doesn't contain metadata and will be ignored.", fn)
            continue
        elif "events" not in json:
            warning("The file doesn't contain event data and will be ignored.", fn)
            continue
        else:
            version = json["meta"].get("file_version")
            if version is None:
                warning(
                    "The file doesn't contain a version (preCICE version v3.2 or earlier) and may be incompatible.",
                    fn,
                )
            elif version != RUN_FILE_VERSION:
                warning(
                    f"The file uses version {version}, which doesn't match the expected version {RUN_FILE_VERSION} and may be incompatible.",
                    fn,
                )

        name = json["meta"]["name"]
        rank = int(json["meta"]["rank"])
        unix_us = int(json["meta"]["unix_us"])
        nameMap, groupedEvents = groupEvents(json["events"], unix_us)
        events.setdefault(name, {})[rank] = {
            "meta": {
                "name": name,
                "rank": rank,
                "size": int(json["meta"]["size"]),
                "unix_us": unix_us,
                "tinit": json["meta"]["tinit"],
            },
            "events": groupedEvents,
            "nameMap": nameMap,
        }

    if not events:
        print("No files loaded")
        sys.exit(1)

    print("Compressing names")
    globalNameMap = compressNames(events)

    return {
        "file_version": MERGED_FILE_VERSION,
        "eventDict": globalNameMap,
        "events": events,
    }


def detectFiles(files: list[pathlib.Path]):
    def searchDir(directory: pathlib.Path):
        assert directory.is_dir()
        import re

        nameMatcher = r".+-\d+-\d+.json"
        return [
            candidate
            for candidate in path.rglob("**/*.json")
            if re.fullmatch(nameMatcher, candidate.name)
        ]

    resolved = []
    for path in files:
        if path.is_file():
            resolved.append(path)
            continue
        if path.is_dir():
            detected = searchDir(path)
            if len(detected) == 0:
                print(f"Nothing found in {path}")
            else:
                print(f"Found {len(detected)} files in {path}")
                resolved += detected
        else:
            print(f'Cannot interpret "{path}"')

    unique = list(set(resolved))
    if len(files) > 1:
        print(f"Found {len(unique)} profiling files in total")
    return unique


def findFilesOfLatestRun(name, sizes):
    assert len(sizes) > 1
    print(f"Found multiple runs for participant {name}")
    timestamps = []
    for size, ranks in sizes.items():
        assert len(ranks) > 0
        example = next(iter(ranks.values()))  # Get some file of this run
        timestamp = int(readRobust(example)["meta"]["unix_us"])
        timestamps.append((size, timestamp))

    # Find oldest size of newest timestamps
    size, _ = max(timestamps, key=lambda p: p[1])
    print(f"`-selected latest run of size {size}")

    return list(sizes[size].values())


def groupRuns(files: list[pathlib.Path]):
    PieceFile = namedtuple("PieceFile", ["name", "rank", "size", "filename"])

    def info(filename: pathlib.Path):
        parts = filename.stem.split("-")
        name = "-".join(parts[:-2])
        return PieceFile(name, int(parts[-2]), int(parts[-1]), filename)

    pieces = [info(filename) for filename in files]

    map = {}
    for n, r, s, fn in pieces:
        rankMap = map.setdefault(n, {}).setdefault(s, {})
        if r in rankMap:
            warning(f"Ignored due to conflict with '{rankMap[r]}'", fn)
        else:
            rankMap.update({r: fn})
    return map


def sanitizeFiles(files: list[pathlib.Path]):
    map = groupRuns(files)
    filesToLoad = []
    for name, sizes in map.items():
        if len(sizes) == 1:
            print(f"Found a single run for participant {name}")
            filesToLoad += [
                filename for _, ranks in sizes.items() for filename in ranks.values()
            ]
            continue

        filesToLoad = findFilesOfLatestRun(name, sizes)
    return filesToLoad


def runMerge(ns):
    return mergeCommand(ns.files, ns.output, not ns.no_align)


def mergeCommand(files, outfile, align):
    resolved = detectFiles(files)
    sanitized = sanitizeFiles(resolved)
    merged = loadProfilingOutputs(sanitized)

    if align:
        merged = alignEvents(merged)

    print(f"Writing to {outfile}")
    if json.__name__ == "orjson":
        data = json.dumps(merged, option=json.OPT_NON_STR_KEYS)
        outfile.write_bytes(data)
    else:
        data = json.dumps(merged)
        outfile.write_text(data)

    return 0


def makeMergeParser(add_help: bool = True):
    merge_help = "Merges preCICE profiling output files to a single file used by the other commands."
    merge = argparse.ArgumentParser(description=merge_help, add_help=add_help)
    merge.add_argument(
        "files",
        type=pathlib.Path,
        nargs="*",
        help="The profiling files to process, directories to search, or nothing to autodetect",
        default=["."],
    )
    merge.add_argument(
        "-o",
        "--output",
        type=pathlib.Path,
        default="profiling.json",
        help="The resulting profiling file.",
    )
    merge.add_argument(
        "-n", "--no-align", action="store_true", help="Don't align participants?"
    )
    return merge


def main():
    parser = makeMergeParser()
    ns = parser.parse_args()
    return runMerge(ns)


if __name__ == "__main__":
    sys.exit(main())
